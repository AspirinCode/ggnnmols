{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11758, 132, 63) (295, 132, 63) (645, 132, 63)\n",
      "(11758, 132, 132, 5) (295, 132, 132, 5) (645, 132, 132, 5)\n",
      "(11758, 12) (295, 12) (645, 12)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.load('data/tox21_graph.npz')\n",
    "atom_feat_tr = data['atom_feat_tr']\n",
    "atom_feat_val = data['atom_feat_val']\n",
    "atom_feat_te = data['atom_feat_te']\n",
    "\n",
    "edge_feat_tr = data['edge_feat_tr']\n",
    "edge_feat_val = data['edge_feat_val']\n",
    "edge_feat_te = data['edge_feat_te']\n",
    "\n",
    "y_tr = data['y_tr']\n",
    "y_val = data['y_val']\n",
    "y_te = data['y_te']\n",
    "\n",
    "y_notnan_tr = data['y_notnan_tr']\n",
    "y_notnan_val = data['y_notnan_val']\n",
    "y_notnan_te = data['y_notnan_te']\n",
    "\n",
    "print(atom_feat_tr.shape, atom_feat_val.shape, atom_feat_te.shape)\n",
    "print(edge_feat_tr.shape, edge_feat_val.shape, edge_feat_te.shape)\n",
    "print(y_tr.shape, y_val.shape, y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/lics/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from functools import partial\n",
    "from ggnnmols.models import GGNN\n",
    "\n",
    "\n",
    "node_dim = atom_feat_tr.shape[-2]\n",
    "node_feat_dim = atom_feat_tr.shape[-1]\n",
    "edge_feat_dim = edge_feat_tr.shape[-1]\n",
    "hidden_size = 512\n",
    "output_dim = y_tr.shape[-1]\n",
    "num_prop = 10\n",
    "\n",
    "model = GGNN(node_dim, node_feat_dim, edge_feat_dim, hidden_size, output_dim, num_prop=num_prop, missing_ys=True)\n",
    "node_inputs = tf.keras.layers.Input(shape=atom_feat_tr.shape[1:])\n",
    "edge_inputs = tf.keras.layers.Input(shape=edge_feat_tr.shape[1:])\n",
    "valid_y_inputs = tf.keras.layers.Input(shape=y_tr.shape[-1:])\n",
    "outputs = model([node_inputs, edge_inputs, valid_y_inputs])\n",
    "\n",
    "def valid_binary_crossentropy(y_true, y_pred, valid_y):\n",
    "    return K.sum(K.binary_crossentropy(y_true, y_pred) * valid_y, axis=-1)\n",
    "\n",
    "def valid_acc(y_true, y_pred, valid_y):\n",
    "    eq_all = K.cast(K.equal(y_true, K.round(y_pred)), K.floatx())\n",
    "    return K.sum(eq_all * valid_y) / K.sum(valid_y)\n",
    "\n",
    "bxen = partial(valid_binary_crossentropy, valid_y=valid_y_inputs)\n",
    "acc = partial(valid_acc, valid_y=valid_y_inputs)\n",
    "acc.__name__ = 'acc'\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=bxen, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11758 samples, validate on 295 samples\n",
      "WARNING:tensorflow:From /home/lics/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.9640 - acc: 0.9247\n",
      "Epoch 00001: val_loss improved from inf to 2.70933, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 283s 24ms/sample - loss: 1.9650 - acc: 0.9246 - val_loss: 2.7093 - val_acc: 0.9210\n",
      "Epoch 2/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.8365 - acc: 0.9273\n",
      "Epoch 00002: val_loss improved from 2.70933 to 2.61908, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 281s 24ms/sample - loss: 1.8362 - acc: 0.9273 - val_loss: 2.6191 - val_acc: 0.9194\n",
      "Epoch 3/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.7754 - acc: 0.9289\n",
      "Epoch 00003: val_loss improved from 2.61908 to 2.54218, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 279s 24ms/sample - loss: 1.7766 - acc: 0.9288 - val_loss: 2.5422 - val_acc: 0.9174\n",
      "Epoch 4/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.7374 - acc: 0.9299\n",
      "Epoch 00004: val_loss did not improve from 2.54218\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.7370 - acc: 0.9300 - val_loss: 2.5652 - val_acc: 0.9173\n",
      "Epoch 5/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.7108 - acc: 0.9301\n",
      "Epoch 00005: val_loss improved from 2.54218 to 2.52882, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.7109 - acc: 0.9301 - val_loss: 2.5288 - val_acc: 0.9206\n",
      "Epoch 6/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.6856 - acc: 0.9310\n",
      "Epoch 00006: val_loss improved from 2.52882 to 2.48673, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 279s 24ms/sample - loss: 1.6849 - acc: 0.9311 - val_loss: 2.4867 - val_acc: 0.9176\n",
      "Epoch 7/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.6644 - acc: 0.9320\n",
      "Epoch 00007: val_loss improved from 2.48673 to 2.41153, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.6638 - acc: 0.9320 - val_loss: 2.4115 - val_acc: 0.9207\n",
      "Epoch 8/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.6073 - acc: 0.9341\n",
      "Epoch 00008: val_loss did not improve from 2.41153\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.6072 - acc: 0.9341 - val_loss: 2.4232 - val_acc: 0.9200\n",
      "Epoch 9/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.5875 - acc: 0.9340\n",
      "Epoch 00009: val_loss did not improve from 2.41153\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.5881 - acc: 0.9340 - val_loss: 2.4634 - val_acc: 0.9180\n",
      "Epoch 10/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.5577 - acc: 0.9354\n",
      "Epoch 00010: val_loss did not improve from 2.41153\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.5577 - acc: 0.9354 - val_loss: 2.4741 - val_acc: 0.9188\n",
      "Epoch 11/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.5262 - acc: 0.9355\n",
      "Epoch 00011: val_loss did not improve from 2.41153\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.5253 - acc: 0.9355 - val_loss: 2.4455 - val_acc: 0.9126\n",
      "Epoch 12/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.4993 - acc: 0.9376\n",
      "Epoch 00012: val_loss did not improve from 2.41153\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.5002 - acc: 0.9375 - val_loss: 2.4657 - val_acc: 0.9184\n",
      "Epoch 13/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.4684 - acc: 0.9382\n",
      "Epoch 00013: val_loss improved from 2.41153 to 2.38800, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.4680 - acc: 0.9383 - val_loss: 2.3880 - val_acc: 0.9189\n",
      "Epoch 14/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.4465 - acc: 0.9391\n",
      "Epoch 00014: val_loss did not improve from 2.38800\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.4470 - acc: 0.9391 - val_loss: 2.4016 - val_acc: 0.9187\n",
      "Epoch 15/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.4198 - acc: 0.9401\n",
      "Epoch 00015: val_loss did not improve from 2.38800\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.4197 - acc: 0.9401 - val_loss: 2.4190 - val_acc: 0.9208\n",
      "Epoch 16/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.4013 - acc: 0.9405\n",
      "Epoch 00016: val_loss did not improve from 2.38800\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.4012 - acc: 0.9405 - val_loss: 2.4503 - val_acc: 0.9162\n",
      "Epoch 17/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.3640 - acc: 0.9420\n",
      "Epoch 00017: val_loss did not improve from 2.38800\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.3634 - acc: 0.9420 - val_loss: 2.4687 - val_acc: 0.9159\n",
      "Epoch 18/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.3428 - acc: 0.9430\n",
      "Epoch 00018: val_loss did not improve from 2.38800\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.3431 - acc: 0.9430 - val_loss: 2.4489 - val_acc: 0.9162\n",
      "Epoch 19/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.3171 - acc: 0.9444\n",
      "Epoch 00019: val_loss did not improve from 2.38800\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.3168 - acc: 0.9444 - val_loss: 2.4308 - val_acc: 0.9192\n",
      "Epoch 20/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.2853 - acc: 0.9452\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.9999998989515007e-05.\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.38800\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.2859 - acc: 0.9452 - val_loss: 2.4137 - val_acc: 0.9181\n",
      "Epoch 21/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.2022 - acc: 0.9485\n",
      "Epoch 00021: val_loss improved from 2.38800 to 2.38412, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.2019 - acc: 0.9486 - val_loss: 2.3841 - val_acc: 0.9197\n",
      "Epoch 22/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.1752 - acc: 0.9500\n",
      "Epoch 00022: val_loss did not improve from 2.38412\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.1757 - acc: 0.9500 - val_loss: 2.4274 - val_acc: 0.9163\n",
      "Epoch 23/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.1584 - acc: 0.9505\n",
      "Epoch 00023: val_loss improved from 2.38412 to 2.37385, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.1589 - acc: 0.9505 - val_loss: 2.3739 - val_acc: 0.9217\n",
      "Epoch 24/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.1415 - acc: 0.9519\n",
      "Epoch 00024: val_loss improved from 2.37385 to 2.30608, saving model to weights.best.hdf5\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.1416 - acc: 0.9519 - val_loss: 2.3061 - val_acc: 0.9194\n",
      "Epoch 25/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.1287 - acc: 0.9525\n",
      "Epoch 00025: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.1292 - acc: 0.9525 - val_loss: 2.3333 - val_acc: 0.9186\n",
      "Epoch 26/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.1126 - acc: 0.9533\n",
      "Epoch 00026: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.1121 - acc: 0.9533 - val_loss: 2.4158 - val_acc: 0.9173\n",
      "Epoch 27/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.0985 - acc: 0.9539\n",
      "Epoch 00027: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.0986 - acc: 0.9538 - val_loss: 2.3406 - val_acc: 0.9210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.0806 - acc: 0.9546\n",
      "Epoch 00028: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.0806 - acc: 0.9546 - val_loss: 2.4270 - val_acc: 0.9121\n",
      "Epoch 29/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.0690 - acc: 0.9552\n",
      "Epoch 00029: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.0685 - acc: 0.9552 - val_loss: 2.3943 - val_acc: 0.9191\n",
      "Epoch 30/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.0564 - acc: 0.9560\n",
      "Epoch 00030: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.0565 - acc: 0.9560 - val_loss: 2.3670 - val_acc: 0.9224\n",
      "Epoch 31/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 1.0438 - acc: 0.9563\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.5999999595806004e-05.\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 1.0435 - acc: 0.9563 - val_loss: 2.3292 - val_acc: 0.9197\n",
      "Epoch 32/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9954 - acc: 0.9591\n",
      "Epoch 00032: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9955 - acc: 0.9591 - val_loss: 2.3706 - val_acc: 0.9187\n",
      "Epoch 33/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9828 - acc: 0.9595\n",
      "Epoch 00033: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9842 - acc: 0.9594 - val_loss: 2.3759 - val_acc: 0.9176\n",
      "Epoch 34/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9775 - acc: 0.9600\n",
      "Epoch 00034: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9780 - acc: 0.9599 - val_loss: 2.3703 - val_acc: 0.9204\n",
      "Epoch 35/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9684 - acc: 0.9605\n",
      "Epoch 00035: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9691 - acc: 0.9605 - val_loss: 2.4072 - val_acc: 0.9203\n",
      "Epoch 36/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9640 - acc: 0.9604\n",
      "Epoch 00036: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9639 - acc: 0.9604 - val_loss: 2.3906 - val_acc: 0.9201\n",
      "Epoch 37/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9585 - acc: 0.9608\n",
      "Epoch 00037: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9584 - acc: 0.9608 - val_loss: 2.3834 - val_acc: 0.9189\n",
      "Epoch 38/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9508 - acc: 0.9610\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.399999983841554e-06.\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9508 - acc: 0.9610 - val_loss: 2.4085 - val_acc: 0.9184\n",
      "Epoch 39/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9295 - acc: 0.9621\n",
      "Epoch 00039: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 279s 24ms/sample - loss: 0.9293 - acc: 0.9621 - val_loss: 2.3994 - val_acc: 0.9178\n",
      "Epoch 40/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9240 - acc: 0.9627\n",
      "Epoch 00040: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9250 - acc: 0.9627 - val_loss: 2.4015 - val_acc: 0.9182\n",
      "Epoch 41/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9228 - acc: 0.9630\n",
      "Epoch 00041: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9225 - acc: 0.9630 - val_loss: 2.3863 - val_acc: 0.9205\n",
      "Epoch 42/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9201 - acc: 0.9627\n",
      "Epoch 00042: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9198 - acc: 0.9627 - val_loss: 2.3887 - val_acc: 0.9200\n",
      "Epoch 43/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9161 - acc: 0.9631\n",
      "Epoch 00043: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9160 - acc: 0.9631 - val_loss: 2.4051 - val_acc: 0.9201\n",
      "Epoch 44/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9136 - acc: 0.9630\n",
      "Epoch 00044: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9130 - acc: 0.9631 - val_loss: 2.3971 - val_acc: 0.9167\n",
      "Epoch 45/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9113 - acc: 0.9632\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 2.5600000299164097e-06.\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9109 - acc: 0.9632 - val_loss: 2.3848 - val_acc: 0.9192\n",
      "Epoch 46/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.9017 - acc: 0.9636\n",
      "Epoch 00046: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.9011 - acc: 0.9637 - val_loss: 2.4020 - val_acc: 0.9186\n",
      "Epoch 47/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.8993 - acc: 0.9638\n",
      "Epoch 00047: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.8998 - acc: 0.9638 - val_loss: 2.3972 - val_acc: 0.9215\n",
      "Epoch 48/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.8982 - acc: 0.9639\n",
      "Epoch 00048: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.8984 - acc: 0.9639 - val_loss: 2.4055 - val_acc: 0.9177\n",
      "Epoch 49/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.8973 - acc: 0.9635\n",
      "Epoch 00049: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.8974 - acc: 0.9635 - val_loss: 2.4100 - val_acc: 0.9175\n",
      "Epoch 50/50\n",
      "11744/11758 [============================>.] - ETA: 0s - loss: 0.8967 - acc: 0.9641\n",
      "Epoch 00050: val_loss did not improve from 2.30608\n",
      "11758/11758 [==============================] - 278s 24ms/sample - loss: 0.8962 - acc: 0.9641 - val_loss: 2.3982 - val_acc: 0.9204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f473eead2e8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "filepath = 'saved_models/weights.best.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='auto', \n",
    "                             save_weights_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor = 0.4,\n",
    "                              patience = 7,\n",
    "                              verbose=1,\n",
    "                              min_lr = 0)\n",
    "\n",
    "\n",
    "model.fit(x=[atom_feat_tr, edge_feat_tr, y_notnan_tr], y=y_tr, \n",
    "          validation_data=([atom_feat_val, edge_feat_val, y_notnan_val], y_val), \n",
    "          epochs=50, batch_size=16, callbacks=[reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
